{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3g9p4a80OO0"
      },
      "source": [
        "# Counterfeit Product Image Detection - Colab Setup\n",
        "\n",
        "This notebook sets up the environment for data collection on Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd71c9d8"
      },
      "source": [
        "## Optional: Authenticate with Hugging Face\n",
        "If you added `HF_TOKEN` to your Colab secrets, run this cell to log in. This prevents rate limit issues and allows access to gated datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "285e8c28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1c56c2b-cbfc-4eb5-9bd0-14920d02b369"
      },
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "try:\n",
        "    # Retrieve token from Secrets\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    # Login\n",
        "    login(token=hf_token)\n",
        "    print(\"\\nâœ“ Successfully logged in to Hugging Face Hub\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Login skipped or failed: {e}\")\n",
        "    print(\"Tip: Add 'HF_TOKEN' in the Secrets tab (ðŸ”‘) on the left if you need authenticated access.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ“ Successfully logged in to Hugging Face Hub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "-V-qAIcSBDrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZHZwCJC0OO2"
      },
      "source": [
        "## Step 1: Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P08Tte7F0OO3",
        "outputId": "17a081d1-a4ff-4d77-d205-0f3c2248b32c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTypfwWg0OO3"
      },
      "source": [
        "## Step 2: Navigate to Project Directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llazpe1k0OO4",
        "outputId": "f40897f1-8fc5-47f4-fc26-2a58e17e9c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Working in: /content/drive/MyDrive/Masters/UIUC_FA2025/CS441/project/Counterfeit-Product-Image-Detection\n",
            "âœ“ Current directory: /content/drive/MyDrive/Masters/UIUC_FA2025/CS441/project/Counterfeit-Product-Image-Detection\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Set project directory (adjust path if needed)\n",
        "PROJECT_DIR = Path('/content/drive/MyDrive/Masters/UIUC_FA2025/CS441/project/Counterfeit-Product-Image-Detection')\n",
        "\n",
        "# Create project directory if it doesn't exist\n",
        "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"Working in: {PROJECT_DIR}\")\n",
        "print(f\"Current directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ICGcubg0OO4"
      },
      "source": [
        "## Step 3: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fclxwf-0OO4",
        "outputId": "8b6f1310-ac92-49ca-daaa-eb7e04a9c3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Dependencies installed (icrawler added)\n"
          ]
        }
      ],
      "source": [
        "%pip install -q icrawler datasets requests pillow\n",
        "\n",
        "print(\"Dependencies installed (icrawler added)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKr5JkmB0OO5"
      },
      "source": [
        "## Step 4: Create Data Directories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R16xFR5L0OO5",
        "outputId": "135bcb70-0cc7-4f5b-d853-4b4db8a8546c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Directory structure created for 10 brands under /content/drive/MyDrive/Masters/UIUC_FA2025/CS441/project/Counterfeit-Product-Image-Detection/data/sneakers\n",
            "  Example: /content/drive/MyDrive/Masters/UIUC_FA2025/CS441/project/Counterfeit-Product-Image-Detection/data/sneakers/authentic/nike\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 target brands\n",
        "BRANDS = [\n",
        "    'nike', 'adidas', 'jordan', 'yeezy', 'new balance',\n",
        "    'converse', 'puma', 'reebok', 'under armour', 'vans'\n",
        "]\n",
        "\n",
        "# Create nested directory structure\n",
        "data_dir = PROJECT_DIR / 'data' / 'sneakers'\n",
        "\n",
        "for brand in BRANDS:\n",
        "    # Create authentic and fake folders for each brand\n",
        "    (data_dir / 'authentic' / brand).mkdir(parents=True, exist_ok=True)\n",
        "    (data_dir / 'fake' / brand).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Directory structure created for {len(BRANDS)} brands under {data_dir}\")\n",
        "print(f\"  Example: {data_dir / 'authentic' / 'nike'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "825LCvsA0OO5"
      },
      "source": [
        "## Step 5: Check Storage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ3CA4S80OO5",
        "outputId": "d37cec86-b5a1-4f74-f404-875709249fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive Storage:\n",
            "  Total: 225.83 GB\n",
            "  Used: 47.65 GB\n",
            "  Free: 178.19 GB\n",
            "\n",
            "Current data folder size: 0.04 GB\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Check Drive storage\n",
        "total, used, free = shutil.disk_usage(\"/content/drive/MyDrive\")\n",
        "print(f\"Google Drive Storage:\")\n",
        "print(f\"  Total: {total / (1024**3):.2f} GB\")\n",
        "print(f\"  Used: {used / (1024**3):.2f} GB\")\n",
        "print(f\"  Free: {free / (1024**3):.2f} GB\")\n",
        "\n",
        "# Check data folder size if it exists\n",
        "data_path = PROJECT_DIR / 'data'\n",
        "if data_path.exists():\n",
        "    total_size = sum(f.stat().st_size for f in data_path.rglob('*') if f.is_file())\n",
        "    print(f\"\\nCurrent data folder size: {total_size / (1024**3):.2f} GB\")\n",
        "else:\n",
        "    print(\"\\nData folder not found (will be created during data collection)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TKl1pnb0OO5"
      },
      "source": [
        "## Step 6: Run Data Collection\n",
        "\n",
        "Now you're ready to run the data collection scripts. They will automatically save to Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V070hWtL0OO6",
        "outputId": "98c4e71f-ac70-4208-c08d-55013e1048c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Unified Scraper (Official/AliExpress/Reddit/iOffer -> Bing Fallback)...\n",
            "\n",
            "[REAL] nike...\n",
            "  - Scraping official site: https://www.nike.com/w?q=sneakers&vst=sneakers...\n",
            ".........................\n",
            "    Got 25 images from official site.\n",
            "  - Target not met (25/30). Scraping 5 via Bing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://sneakerbardetroit.com/wp-content/uploads/2023/01/Nike-Dunk-Low-Chicago-Split-University-Red-DZ2536-600-Release-Date.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.sportsdirect.com/images/imgzoom/13/13120641_xxl.jpg, error: HTTPSConnectionPool(host='www.sportsdirect.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] adidas...\n",
            "  - Scraping official site: https://www.adidas.com/us/men-athletic_sneakers...\n",
            "\n",
            "    Got 0 images from official site.\n",
            "  - Target not met (0/30). Scraping 30 via Bing...\n",
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] jordan...\n",
            "  - Scraping official site: https://www.nike.com/w/mens-jordan-shoes-37eefznik1zy7ok...\n",
            ".........................\n",
            "    Got 25 images from official site.\n",
            "  - Target not met (25/30). Scraping 5 via Bing...\n",
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] yeezy...\n",
            "  - Scraping official site: https://www.shoebacca.com/collections/adidas-yeezy...\n",
            "..\n",
            "    Got 2 images from official site.\n",
            "  - Target not met (2/30). Scraping 28 via Bing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://sneakerbardetroit.com/wp-content/uploads/2020/12/adidas-Yeezy-Boost-350-V2-Ash-Blue.jpg\n",
            "ERROR:downloader:Response status code 403, file https://sneakerbardetroit.com/wp-content/uploads/2017/03/adidas-yeezy-boost-350-v2-zebra.jpg\n",
            "ERROR:downloader:Response status code 403, file https://sneakerbardetroit.com/wp-content/uploads/2020/01/adidas-Yeezy-Boost-350-V2-Earth-FX9033-Release-Date-On-Feet-6.jpg\n",
            "ERROR:downloader:Response status code 404, file https://image.goat.com/transform/v1/attachments/product_template_additional_pictures/images/075/775/178/original/924555_01.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] new balance...\n",
            "  - Scraping official site: https://www.newbalance.com/shoes/?searchKey=sneakers&sm=Search%20Bar%20and%20Type%20Text...\n",
            "\n",
            "    Got 0 images from official site.\n",
            "  - Target not met (0/30). Scraping 30 via Bing...\n",
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] converse...\n",
            "  - Scraping official site: https://www.converse.com/search?q=sneakers...\n",
            "\n",
            "    Got 0 images from official site.\n",
            "  - Target not met (0/30). Scraping 30 via Bing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://www.converse.com/dw/image/v2/BCZC_PRD/on/demandware.static/-/Sites-cnv-master-catalog/default/dw3d987bc4/images/a_107/560845C_A_107X1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.converse.com.au/media/wysiwyg/19622_L_107X1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.converse.com/on/demandware.static/-/Library-Sites-SharedLibrary/default/dwaa9aecdd/firstspirit/media/store_locator___details/gbl_store_locator/keep_it_classic/2020_07_17/D-FA20-Store-classic-shoes.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.converse.com/dw/image/v2/BJJF_PRD/on/demandware.static/-/Sites-cnv-master-catalog-we/default/dw5fd10496/images/k_08/162050C_K_08X1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://www.converse.com/dw/image/v2/BCZC_PRD/on/demandware.static/-/Sites-cnv-master-catalog/default/dw8711f6cf/images/d_08/M9160_D_08X1.jpg\n",
            "ERROR:downloader:Response status code 403, file https://cdn.laredoute.com/products/f/e/e/fee40f123948ede23fe0a6d648ce3067.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] puma...\n",
            "  - Scraping official site: https://us.puma.com/us/en/search?q=Sneakers&offset=24...\n",
            "..............................\n",
            "    Got 30 images from official site.\n",
            "\n",
            "[REAL] reebok...\n",
            "  - Scraping official site: https://www.reebok.com/pages/search-results?q=sneakers&current=2...\n",
            ".............................\n",
            "    Got 29 images from official site.\n",
            "  - Target not met (29/30). Scraping 1 via Bing...\n",
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] under armour...\n",
            "  - Scraping official site: https://www.underarmour.com/en-us/c/mens/shoes/running/...\n",
            "\n",
            "    Got 0 images from official site.\n",
            "  - Target not met (0/30). Scraping 30 via Bing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/011/773/363/original/preposition-of-place-illustration-little-girl-sitting-on-and-under-the-table-english-vocabulary-words-flashcard-set-for-education-vector.jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/011/773/367/non_2x/preposition-of-place-illustration-little-boy-sitting-on-and-under-the-table-english-vocabulary-words-flashcard-set-for-education-vector.jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/021/159/365/original/under-preposition-english-line-icon-illustration-vector.jpg\n",
            "ERROR:downloader:Response status code 403, file https://static.vecteezy.com/system/resources/previews/016/558/276/large_2x/underwater-scene-ocean-coral-reef-underwater-sea-world-under-water-background-free-photo.jpg\n",
            "ERROR:downloader:Response status code 400, file https://media.istockphoto.com/id/1420435705/vector/preposition-of-place-with-cartoon-girl-and-a-table.jpg\n",
            "ERROR:downloader:Exception caught when downloading file https://www.sportsdirect.com/images/imgzoom/15/15137040_xxl.jpg, error: HTTPSConnectionPool(host='www.sportsdirect.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n",
            "ERROR:downloader:Exception caught when downloading file https://www.sportsdirect.com/images/imgzoom/15/15137040_xxl.jpg, error: HTTPSConnectionPool(host='www.sportsdirect.com', port=443): Read timed out. (read timeout=5), remaining retry times: 1\n",
            "ERROR:downloader:Exception caught when downloading file https://www.sportsdirect.com/images/imgzoom/15/15137040_xxl.jpg, error: HTTPSConnectionPool(host='www.sportsdirect.com', port=443): Read timed out. (read timeout=5), remaining retry times: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Bing crawler finished.\n",
            "\n",
            "[REAL] vans...\n",
            "  - Scraping official site: https://www.vans.com/en-us/c/mens/shoes-1100?icn=subnav&filters={%22Product+Category%22:[%22Sneakers%22]}&sort=bestMatches...\n",
            "\n",
            "    Got 0 images from official site.\n",
            "  - Target not met (0/30). Scraping 30 via Bing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:downloader:Exception caught when downloading file https://www.sportsdirect.com/images/imgzoom/24/24628018_xxl_a2.jpg, error: HTTPSConnectionPool(host='www.sportsdirect.com', port=443): Read timed out. (read timeout=5), remaining retry times: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Bing crawler finished.\n",
            "\n",
            "âœ“ Collection process finished\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import urllib.parse\n",
        "from pathlib import Path\n",
        "from icrawler.builtin import BingImageCrawler\n",
        "import logging\n",
        "import re\n",
        "\n",
        "# Suppress verbose icrawler logs\n",
        "logging.getLogger('icrawler').setLevel(logging.ERROR)\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure BRANDS and data_dir are defined from previous cells\n",
        "# BRANDS = [...]\n",
        "# data_dir = ...\n",
        "\n",
        "NUM_REAL = 30\n",
        "NUM_FAKE = 10\n",
        "\n",
        "# URLs for authentic sites\n",
        "OFFICIAL_SITES = {\n",
        "    'nike': 'https://www.nike.com/w?q=sneakers&vst=sneakers',\n",
        "    'adidas': 'https://www.adidas.com/us/men-athletic_sneakers',\n",
        "    'jordan': 'https://www.nike.com/w/mens-jordan-shoes-37eefznik1zy7ok',\n",
        "    'yeezy': 'https://www.shoebacca.com/collections/adidas-yeezy',\n",
        "    'new balance': 'https://www.newbalance.com/shoes/?searchKey=sneakers&sm=Search%20Bar%20and%20Type%20Text',\n",
        "    'converse': 'https://www.converse.com/search?q=sneakers',\n",
        "    'puma': 'https://us.puma.com/us/en/search?q=Sneakers&offset=24',\n",
        "    'reebok': 'https://www.reebok.com/pages/search-results?q=sneakers&current=2',\n",
        "    'under armour': 'https://www.underarmour.com/en-us/c/mens/shoes/running/',\n",
        "    'vans': 'https://www.vans.com/en-us/c/mens/shoes-1100?icn=subnav&filters={%22Product+Category%22:[%22Sneakers%22]}&sort=bestMatches'\n",
        "}\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def download_single_image(src, save_path):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "        # Increased timeout slightly\n",
        "        response = requests.get(src, timeout=15, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            # Check content type or extension\n",
        "            content_type = response.headers.get(\"content-type\", \"\")\n",
        "            if \"image\" in content_type or str(save_path).endswith(\".jpg\"):\n",
        "                with open(save_path, \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "# --- 1. Real Scraper (Official + Bing Fallback) ---\n",
        "def scrape_real_images(brand):\n",
        "    print(f\"\\n[REAL] {brand}...\")\n",
        "    save_dir = data_dir / 'authentic' / brand\n",
        "\n",
        "    # Clean directory first\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    # A. Try Official Site First\n",
        "    url = OFFICIAL_SITES.get(brand)\n",
        "    if url:\n",
        "        print(f\"  - Scraping official site: {url}...\")\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "            }\n",
        "            page = requests.get(url, headers=headers, timeout=15)\n",
        "            soup = BeautifulSoup(page.text, 'html.parser')\n",
        "            images = soup.find_all(\"img\")\n",
        "\n",
        "            # Regex for price like $100 or $100.99\n",
        "            price_pattern = re.compile(r'[\\$â‚¬Â£]\\s*(\\d+(?:\\.\\d{2})?)')\n",
        "\n",
        "            for img in images:\n",
        "                if count >= NUM_REAL: break\n",
        "\n",
        "                src = img.get(\"src\") or img.get(\"data-src\")\n",
        "                if not src: continue\n",
        "\n",
        "                # Filter small icons/logos\n",
        "                if \"logo\" in src.lower() or \"icon\" in src.lower() or \".svg\" in src.lower():\n",
        "                    continue\n",
        "\n",
        "                # Attempt to find price in parent containers\n",
        "                price_str = \"0_00\"\n",
        "                try:\n",
        "                    curr = img\n",
        "                    # Traverse up to 4 parents to find a price\n",
        "                    for _ in range(4):\n",
        "                        if curr.parent:\n",
        "                            curr = curr.parent\n",
        "                            txt = curr.get_text()\n",
        "                            match = price_pattern.search(txt)\n",
        "                            if match:\n",
        "                                # Format: 105.99 -> 105_99\n",
        "                                price_str = match.group(1).replace('.', '_')\n",
        "                                break\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                # Absolute URL\n",
        "                if src.startswith(\"//\"):\n",
        "                    src = \"https:\" + src\n",
        "                elif src.startswith(\"/\"):\n",
        "                    # Handle relative URLs\n",
        "                    parsed_url = urllib.parse.urlparse(url)\n",
        "                    base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
        "                    src = urllib.parse.urljoin(base_url, src)\n",
        "\n",
        "                # Download with price in filename\n",
        "                # Format: brand_index_price.jpg\n",
        "                filename = save_dir / f\"{brand}_{count}_{price_str}.jpg\"\n",
        "                if download_single_image(src, filename):\n",
        "                    count += 1\n",
        "                    print(\".\", end=\"\")\n",
        "            print(f\"\\n    Got {count} images from official site.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Error scraping official site: {e}\")\n",
        "\n",
        "    # B. Fallback to Bing (if count < NUM_REAL)\n",
        "    if count < NUM_REAL:\n",
        "        needed = NUM_REAL - count\n",
        "        print(f\"  - Target not met ({count}/{NUM_REAL}). Scraping {needed} via Bing...\")\n",
        "        try:\n",
        "            crawler = BingImageCrawler(storage={'root_dir': str(save_dir)}, downloader_threads=4)\n",
        "            # IMPROVED QUERY: added 'footwear' and 'product' to disambiguate 'under armour' from 'under the table'\n",
        "            query = f\"{brand} sneakers footwear product photo white background\"\n",
        "\n",
        "            # file_idx_offset='auto' ensures we don't overwrite if filenames differ\n",
        "            crawler.crawl(keyword=query, max_num=needed, file_idx_offset='auto')\n",
        "            print(f\"    Bing crawler finished.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    Error scraping Bing: {e}\")\n",
        "\n",
        "\n",
        "# --- 2. Fake Scraper (AliExpress + Reddit + iOffer + Bing Fallback) ---\n",
        "def scrape_aliexpress(brand):\n",
        "    print(f\"  - Searching AliExpress for '{brand}'...\")\n",
        "    query = urllib.parse.quote(f\"{brand} sneakers\")\n",
        "    url = f\"https://www.aliexpress.com/wholesale?SearchText={query}&g=y\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
        "        \"Referer\": \"https://www.google.com/\"\n",
        "    }\n",
        "\n",
        "    found_imgs = set()\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=12)\n",
        "        if resp.status_code == 200:\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            for img in soup.find_all(\"img\"):\n",
        "                raw = img.get(\"src\") or img.get(\"data-src\") or \"\"\n",
        "                if not raw or \"data:image\" in raw or \"base64\" in raw: continue\n",
        "                if raw.startswith(\"//\"): raw = \"https:\" + raw\n",
        "\n",
        "                # Loose filter for product images\n",
        "                if \".jpg\" in raw or \".png\" in raw:\n",
        "                    found_imgs.add(raw)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return list(found_imgs)\n",
        "\n",
        "def scrape_reddit_bs4(brand):\n",
        "    print(f\"  - Searching Reddit for '{brand}'...\")\n",
        "    url = f\"https://old.reddit.com/r/Repsneakers/search?q={brand}+sneakers&restrict_sr=on&include_over_18=on&sort=relevance&t=all\"\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    images = []\n",
        "    try:\n",
        "        resp = requests.get(url, headers=headers, timeout=10)\n",
        "        if resp.status_code == 200:\n",
        "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "            for img in soup.find_all(\"img\"):\n",
        "                src = img.get(\"src\")\n",
        "                if src and \"preview\" in src and \"redd.it\" in src:\n",
        "                    images.append(src)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return images\n",
        "\n",
        "def scrape_ioffer(brand):\n",
        "    # Robust iOffer scraper as requested\n",
        "    print(f\"  - Searching iOffer for '{brand}'...\")\n",
        "\n",
        "    # Updated URL structure based on user feedback\n",
        "    base_url = \"https://www.ioffer.com/products_s\"\n",
        "    keywords = urllib.parse.quote_plus(f\"{brand} sneakers\")\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\"\n",
        "    }\n",
        "\n",
        "    valid_urls = []\n",
        "\n",
        "    # Iterate through pagination (limit to 2 pages)\n",
        "    for page in range(1, 3):\n",
        "        # Format: https://www.ioffer.com/products_s?dropdown_selector=%23bottom_products_search_dropdown&keywords=nike+sneakers\n",
        "        target_url = f\"{base_url}?dropdown_selector=%23bottom_products_search_dropdown&keywords={keywords}&page={page}\"\n",
        "\n",
        "        try:\n",
        "            # Handle redirects and timeouts\n",
        "            resp = requests.get(target_url, headers=headers, timeout=10, allow_redirects=True)\n",
        "\n",
        "            # Check if page is valid\n",
        "            if resp.status_code != 200:\n",
        "                print(f\"    [Log] iOffer Page {page} failed: Status {resp.status_code}\")\n",
        "                continue\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
        "\n",
        "            # Find listing images (generic approach)\n",
        "            images = soup.find_all(\"img\")\n",
        "\n",
        "            for img in images:\n",
        "                src = img.get('src')\n",
        "                if not src: continue\n",
        "\n",
        "                # Filter out obvious UI elements\n",
        "                if \"logo\" in src or \"pixel\" in src or \"blank\" in src:\n",
        "                    continue\n",
        "\n",
        "                if src.startswith(\"//\"): src = \"https:\" + src\n",
        "                if not src.startswith(\"http\"): continue\n",
        "\n",
        "                # Quick validity check (HEAD request)\n",
        "                try:\n",
        "                    head = requests.head(src, headers=headers, timeout=3)\n",
        "                    if head.status_code == 200:\n",
        "                        valid_urls.append(src)\n",
        "                    else:\n",
        "                        print(f\"    [Log] Discarding broken image: {src} (Status {head.status_code})\")\n",
        "                except Exception:\n",
        "                    print(f\"    [Log] Validation failed for image: {src}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    [Log] iOffer scraping error on page {page}: {e}\")\n",
        "            break\n",
        "\n",
        "    return list(set(valid_urls))\n",
        "\n",
        "def scrape_fake_images(brand):\n",
        "    print(f\"\\n[FAKE] {brand}...\")\n",
        "    save_dir = data_dir / 'fake' / brand\n",
        "\n",
        "    # Clean directory first\n",
        "    if save_dir.exists():\n",
        "        shutil.rmtree(save_dir)\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1. Gather URLs from multiple sources\n",
        "    imgs = scrape_aliexpress(brand)\n",
        "\n",
        "    # Add Reddit if needed\n",
        "    if len(imgs) < NUM_FAKE:\n",
        "        imgs += scrape_reddit_bs4(brand)\n",
        "\n",
        "    # Add iOffer if needed (Combined as requested)\n",
        "    if len(imgs) < NUM_FAKE:\n",
        "        imgs += scrape_ioffer(brand)\n",
        "\n",
        "    # 2. Download\n",
        "    count = 0\n",
        "    seen = set()\n",
        "    for url in imgs:\n",
        "        if count >= NUM_FAKE: break\n",
        "        if url in seen: continue\n",
        "        seen.add(url)\n",
        "\n",
        "        path = save_dir / f\"{brand}_fake_{count}.jpg\"\n",
        "        if download_single_image(url, path):\n",
        "            count += 1\n",
        "            print(\".\", end=\"\")\n",
        "\n",
        "    print(f\"\\n    Downloaded {count} fake images manually.\")\n",
        "\n",
        "    # 3. Bing Fallback\n",
        "    if count < NUM_FAKE:\n",
        "        print(f\"  - Target not met. Running Bing Crawler fallback...\")\n",
        "        try:\n",
        "            crawler = BingImageCrawler(storage={'root_dir': str(save_dir)}, downloader_threads=4)\n",
        "            q = f\"{brand} fake sneakers replica\"\n",
        "            crawler.crawl(keyword=q, max_num=NUM_FAKE - count, file_idx_offset='auto')\n",
        "        except Exception as e:\n",
        "            print(f\"    Bing fallback failed: {e}\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "print(\"Starting Unified Scraper (Official/AliExpress/Reddit/iOffer -> Bing Fallback)...\")\n",
        "for brand in BRANDS:\n",
        "    scrape_real_images(brand)\n",
        "    # scrape_fake_images(brand)\n",
        "\n",
        "print(\"\\nâœ“ Collection process finished\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcFUi9287xMw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}